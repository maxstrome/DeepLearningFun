# -*- coding: utf-8 -*-
"""EBM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZlAboSReFO7t1P_ZidspO-KeEguOZ2mH
"""

# Standard imports
import jax
from jax import numpy as jnp
import matplotlib.pyplot as plt

# Feel free to pick your favorite number :)
key = jax.random.PRNGKey(12)

# First we set up the guassian blob dataset, note that the data in not symmetric in size
key = jax.random.PRNGKey(12)
key, subkey1, subkey2 = jax.random.split(key, 3)

data_set_size = 2_000
data = jnp.concatenate((-0.5 + 0.2 * jax.random.normal(subkey1, (int(data_set_size * 1/5), 2)),
                        0.5 + 0.2 * jax.random.normal(subkey2, (int(data_set_size * 4/5), 2))), axis=0)

# data_set_size = 500
# data = (-0.5 + 0.2 * jax.random.normal(subkey1, (int(data_set_size), 2)))
print(data.shape)

# Plot the data points
plt.scatter(data[:, 0], data[:, 1], s=3, label="Data")
plt.legend()
plt.show()

"""# Training

Now is where you come in! Put your neural network, SSM, DSM, training, and logging code below!
"""

!pip install wandb  > /dev/null

import wandb

wandb.init(project="ebm_training_extropic", entity="mstrome")

import jax
from jax import numpy as jnp
import matplotlib.pyplot as plt
from flax import linen as nn
import optax
import time

# Define basic model
class EBM(nn.Module):
    @nn.compact # using compact for readability
    def __call__(self, x):
        x = nn.Dense(features=128)(x)
        x = nn.softplus(x)
        x = nn.Dense(features=128)(x)
        x = nn.softplus(x)
        x = nn.Dense(features=1)(x)
        return x

# Define data loader
def data_loader(data, batch_size=64, shuffle=True, seed=None):
    data_set_size = data.shape[0]
    indices = jnp.arange(data_set_size)
    if shuffle:
        if seed is None:
            seed = int(time.time())
        indices = jax.random.permutation(jax.random.PRNGKey(seed), indices)

    for start_idx in range(0, data_set_size, batch_size):
        end_idx = start_idx + batch_size
        batch_indices = indices[start_idx:end_idx]
        yield data[batch_indices]

def dsm_loss_fn(model, params, x, key, sigma=0.1):

    def energy_fn(inp) -> float:
      energy = model.apply(params, inp)
      return jnp.squeeze(energy)

    key, subkey = jax.random.split(key)
    x_noisy = x + sigma * jax.random.normal(subkey, x.shape)
    score_fn = jax.grad(energy_fn)
    batch_grad_energy_fn = jax.vmap(score_fn, in_axes=0, out_axes=0)

    noisy_score = batch_grad_energy_fn(x_noisy)
    grad_log_q = (x - x_noisy) / (sigma ** 2)

    # Compute the DSM loss
    dsm_loss = (1/2)*jnp.mean(jnp.sum((noisy_score - grad_log_q) ** 2, axis=1))

    return dsm_loss

def ssm_loss_fn(model, params, x, key, n_slices=100):
    def energy_fn(inp):
        return jnp.squeeze(model.apply(params, inp))

    def score_fn(inp):
        return jax.grad(energy_fn)(inp)

    def hessian_fn(inp): # did hessian for simplicity, could optimize using Hessian Vector Product formula found in https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html
      return jax.hessian(energy_fn)(inp)

    def vector_hessian_vector(hess, vec):
      return vec @ hess @ vec.T

    # Create random projection vectors
    key, subkey = jax.random.split(key)
    v = jax.random.normal(subkey, (n_slices,) + x.shape[1:])
    # Normalize the random vectors
    v /= jnp.linalg.norm(v, axis=-1, keepdims=True)

    # Compute the score
    scores = jax.vmap(score_fn)(x)
    hessians = jax.vmap(hessian_fn)(x)
    vhv = jax.vmap(lambda v_single: jax.vmap(lambda hess_single: vector_hessian_vector(vec=v_single, hess=hess_single))(hessians))(v)
    term1 = jnp.mean(jnp.sum(vhv, axis=0))
    term2 = 0.5 * jnp.mean(jnp.sum((scores@v.T) ** 2, axis=1))

    # Compute the final SSM loss
    ssm_loss = jnp.mean(term1 + term2)

    return ssm_loss

x = jnp.ones((1, 2))
model = EBM()
params = model.init(jax.random.key(0), x)
dsm_loss_fn(EBM(), params, next(data_loader(data=data, batch_size=3, shuffle=True)), jax.random.PRNGKey(12))
ssm_loss_fn(EBM(), params, next(data_loader(data=data, batch_size=3, shuffle=True)), jax.random.PRNGKey(12))

def train_step(params, opt_state, x_batch, loss_fn, optimizer, subkey):

    def loss(params, x):
        return loss_fn(EBM(), params, x, subkey)

    grad_fn = jax.value_and_grad(loss)
    loss_val, grads = grad_fn(params, x_batch)
    updates, opt_state = optimizer.update(grads, opt_state)

    # Apply the computed updates to the model parameters.
    params = optax.apply_updates(params, updates)

    # Return the updated model parameters, the updated optimizer state, and the computed loss value for the batch.
    return params, opt_state, loss_val

def train_model(loss_fn, loss_name: str, params: optax.Params, opt_state: optax.OptState, optimizer: optax.GradientTransformation, epochs=10, batch_size=64):
    key = jax.random.PRNGKey(12)
    for epoch in range(epochs):
        for x_batch in data_loader(data, batch_size):
            key, subkey = jax.random.split(key)
            params, opt_state, loss_val = train_step(params, opt_state, x_batch, loss_fn, optimizer, subkey)
            wandb.log({loss_name: loss_val.item()})
        if epoch % 10 == 0:
          print(f"Epoch {epoch + 1}, Loss: {loss_val.item()}")
    return params

x = jnp.ones((1, 2))
model = EBM()
params = model.init(jax.random.key(0), x)
optimizer = optax.adam(learning_rate=1e-3)
opt_state = optimizer.init(params)
params = train_model(ssm_loss_fn, epochs=500, batch_size=256, loss_name="SSM", params=params, opt_state=opt_state, optimizer=optimizer)

def inference(inp):
  energy = model.apply(params, inp)
  return -jnp.squeeze(energy)

# Plotting utilities
def get_energy_map(model, density=100, val=1.6):
    xs = jnp.linspace(-val, val, density)
    ys = jnp.linspace(-val, val, density)
    X, Y = jnp.meshgrid(xs, ys)
    XY = jnp.stack([X, Y], axis=-1)
    XY = XY.reshape(-1, 2)
    f_vectorized = jax.vmap(model, in_axes=(0))
    Z = f_vectorized(XY)
    Z = Z.reshape(100, 100)
    return X, Y, Z

def get_score_map(model, density=30, val=1.6):
    xs = jnp.linspace(-val, val, density)
    ys = jnp.linspace(-val, val, density)
    X, Y = jnp.meshgrid(xs, ys)
    XY = jnp.stack([X, Y], axis=-1)
    XY = XY.reshape(-1, 2)
    f_vectorized = jax.vmap(jax.grad(lambda x: -1 * model(x)), in_axes=(0))
    Z = f_vectorized(XY)
    Z = Z.reshape(density, density, 2)

    return X, Y, Z

# Now plot your model here! Replace `ebm` with whatever your inference function is named
X, Y, Z = get_score_map(inference)
plt.figure(figsize=(10, 10))
plt.quiver(X, Y, Z[:,:,0], Z[:,:,1], color='g')
plt.scatter(data[:, 0], data[:, 1], s=3, label="Data")
plt.show()


X, Y, Z = get_energy_map(inference)
plt.figure(figsize=(10, 10))
plt.contourf(X, Y, Z, 50, cmap='viridis')
plt.colorbar()
plt.scatter(data[:, 0], data[:, 1], s=3, label="Data")
plt.show()